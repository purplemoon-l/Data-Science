{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PolFe8GQqhFD"
      },
      "source": [
        "# Foundations of Data Science\n",
        "## Homework 5: Algorithmic fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LlRkZpuqhFF"
      },
      "source": [
        "Student Name: \n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTrbgxPwqhFG"
      },
      "source": [
        "### Part 1: Algorithmic fairness (15 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ_ksbx1qhFG"
      },
      "source": [
        "#### Data acquisition and preparation (4 points)\n",
        "\n",
        "For this question we will use the \"Adult\" dataset from the UC Irvine repository.\n",
        "\n",
        "This data is from the United States census, and we will examine the algorithmic fairness for an income prediction task. For more information about the dataset, see [Here](https://archive.ics.uci.edu/ml/datasets/adult).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPrkBq0QqhFG"
      },
      "source": [
        "1\\. Download the data. (1 point)\n",
        "\n",
        "Load data from the URL using the pandas read_csv method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eo3T8WthqhFH"
      },
      "outputs": [],
      "source": [
        "#Place code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWv9MhmBqhFH"
      },
      "source": [
        "2\\. If the column headers are not correct, assign names to them (hint: use the readme from the source website). Compute descriptive statistics for the education level. (2 points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w0YKpAaqhFI"
      },
      "outputs": [],
      "source": [
        "#Place code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzW72jtgqhFI"
      },
      "source": [
        "3\\. Select one attribute as protected. Explain the reason why you selected this attribute. (1 point)\n",
        "\n",
        "Protected attributes require the prefix protected. The outcome attribute requires the prefix target. For example, if you need to measure fairness rankings of a dataset with the columns sex and credit_score, rename the columns to protected_sex and target_credit_Score. Update the column names for our dataset (hint: you may also have to convert the target to a binary variable and create dummy variables for those that are categorical, for upcoming steps). (1 point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtX2hGMuqhFI"
      },
      "outputs": [],
      "source": [
        "#Place code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AJEeLM3qhFI"
      },
      "source": [
        "#### Build a Classifier (5 points)\n",
        "\n",
        "4\\. Select a type of classifier to build for the income prediction task. Give reasoning for why you picked this type. (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xafdEVUhqhFI"
      },
      "source": [
        "$\\color{blue}{\\text{ Place your answer here.}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug-z4UtsqhFI"
      },
      "source": [
        "5\\. Split the data into training and testing. Use pandas to create two data frames: train_df and test_df, where train_df has 80% of the data chosen uniformly at random without replacement (test_df should have the other 20%). Also, make sure to write your own code to do the splits. You may use any random() function numpy but do not use the data splitting functions from Sklearn. (1 point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Gvo-CLqhFI"
      },
      "outputs": [],
      "source": [
        "#Place code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDo8HGSVqhFJ"
      },
      "source": [
        "6\\. On the training set, implement your classifier. Give reasoning for your choice of any hyperparameter(s). (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35vizJXbqhFJ"
      },
      "outputs": [],
      "source": [
        "#Place code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETsYTrEAqhFJ"
      },
      "source": [
        "7\\. To demonstrate the performance of your classifier, we will now plot the AUROC. Below are two functions which you can use. What you need to add is code to plot the AUROC for all the data and as well for each value of the protected attribute (on one set of axes). (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0I8dyzaqhFJ"
      },
      "outputs": [],
      "source": [
        "#Code for computing the AUCROC\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "\n",
        "def getAUC(truth, pred):\n",
        "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
        "    return auc(fpr, tpr)\n",
        "\n",
        "\n",
        "def plotAUC(truth, pred, lab):\n",
        "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
        "    plt.plot(fpr, tpr, color=c, label= lab+' (AUC = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "#Add code below to subset rows by protected attribute\n",
        "\n",
        "#Add code below to make the plot    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AzRqWq0qhFJ"
      },
      "source": [
        "#### Assessing algorithmic fairness (5 points)\n",
        "\n",
        "8\\. Algorithmic Fairness metrics (2 points) \n",
        "\n",
        "Pick a fairness metric to apply to the income prediction task and your selected protected attribute. Explain why you selected this metric. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV-xH4L-qhFJ"
      },
      "source": [
        "$\\color{blue}{\\text{ Place your answer here.}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPo8FVm7qhFJ"
      },
      "source": [
        "9\\. Compute the chosen metric for your protected attribute. Hint: this will require you to first find the threshold wiith the best when predicting on the entire data, and then computing the TPR/FPR or other necessary parameters at that threshold for each value of the protected attribute. (2 points)\n",
        "\n",
        "There are many ways we could locate the threshold with the optimal balance between the false positive rate (FPR) and true positive rate (TPR).\n",
        "\n",
        "As a reminder, the TPR is called the Sensitivity. The inverse of the false-positive rate (1-FPR) is called the Specificity.\n",
        "\n",
        "<center>Sensitivity = $\\frac{TP}{TP + FN}$\n",
        "Specificity = $\\frac{TN}{FP + TN}$</center>\n",
        "\n",
        "where:\n",
        "\n",
        "<center>Sensitivity = TPR and Specificity = 1 â€“ FPR</center>\n",
        "\n",
        "The Geometric Mean or g-mean is a metric for imbalanced classification that, if optimized, will seek a balance between the sensitivity and the specificity.\n",
        "\n",
        "<center>g-mean = $\\sqrt{Sensitivity * Specificity}$ </center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFSSrIjvqhFK"
      },
      "outputs": [],
      "source": [
        "#Place code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpHbXceVqhFK"
      },
      "source": [
        "10\\. Is there a disparity? How can we interpret its magnitude? Is such a disparity a bad thing/avoidable? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY0-cGaIqhFL"
      },
      "source": [
        "$\\color{blue}{\\text{ Place your answer here.}}$"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}